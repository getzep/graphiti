{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9i9uUZ3pWQE",
    "outputId": "84404bb8-5841-4f2f-dd87-7f909c6e95aa"
   },
   "outputs": [],
   "source": [
    "######## Installations - BE SURE TO MAKE YOUR OWN LOCAL VENV FIRST\n",
    "\n",
    "%pip install pandas graphiti-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgzveXyAp35v"
   },
   "outputs": [],
   "source": [
    "######## Imports\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from utils import dict_to_entity, entity_to_dict, ingest_and_label_minidataset\n",
    "\n",
    "from graphiti_core.llm_client import OpenAIClient\n",
    "from graphiti_core.llm_client.config import LLMConfig\n",
    "from graphiti_core.nodes import EntityNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RjEZnk5v530"
   },
   "outputs": [],
   "source": [
    "######## Load the eval dataset\n",
    "folder_path = './longmemeval_data/snippetized_data'\n",
    "file_path = os.path.join(folder_path, 'longmemeval_oracle_snippetized.csv')\n",
    "lme_dataset_df = pd.read_csv(file_path)\n",
    "lme_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Methods\n",
    "\n",
    "MAX_NUM_PREVIOUS_MESSAGES = 5\n",
    "\n",
    "\n",
    "def filter_for_zep_labelling(df):\n",
    "    \"\"\"\n",
    "    Filters the dataset for the snippets we want to use for Zep labelling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter to only rows where question_type = single_session_user\n",
    "    df = df[df['question_type'] == 'single-session-user']\n",
    "\n",
    "    # Filter only where message_has_answer = True\n",
    "    df = df[df['message_has_answer'] == True]\n",
    "\n",
    "    # Filter to only rows where num_previous_messages = 5\n",
    "    df = df[df['num_previous_messages'] == 5]\n",
    "\n",
    "    # Limit to only 5 rows\n",
    "    df = df.head(1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_previous_messages(df):\n",
    "    \"\"\"\n",
    "    Expands the previous_messages column into separate columns.\n",
    "    \"\"\"\n",
    "    # First parse the string into actual list of dicts\n",
    "    df['previous_messages'] = df['previous_messages'].apply(json.loads)\n",
    "\n",
    "    # Then create separate columns for each message\n",
    "    for i in range(MAX_NUM_PREVIOUS_MESSAGES):\n",
    "        df[f'previous_message_{i + 1}'] = df['previous_messages'].apply(\n",
    "            lambda x: x[i] if i < len(x) else None\n",
    "        )\n",
    "\n",
    "    # Drop the original previous_messages column if desired\n",
    "    return df.drop('previous_messages', axis=1)\n",
    "\n",
    "\n",
    "def make_messages_readable(df):\n",
    "    \"\"\"\n",
    "    Makes the messages more readable.\n",
    "    \"\"\"\n",
    "    for i in range(MAX_NUM_PREVIOUS_MESSAGES):\n",
    "        df[f'previous_message_{i + 1}'] = df[f'previous_message_{i + 1}'].apply(\n",
    "            lambda x: '|' * 10 + f\"  {x['role']}  \" + '|' * 10 + '\\n\\n' + f\"{x['content']}\"\n",
    "            if x is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    df['message'] = df.apply(\n",
    "        lambda row: '|' * 10\n",
    "        + f\"  {row['message_role']}  \"\n",
    "        + '|' * 10\n",
    "        + '\\n\\n'\n",
    "        + f\"{row['message']}\"\n",
    "        if row['message'] is not None\n",
    "        else None,\n",
    "        axis=1,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def order_columns(df):\n",
    "    \"\"\"\n",
    "    Orders the columns in the way we want them.\n",
    "    \"\"\"\n",
    "    df = df[\n",
    "        [\n",
    "            'question_id',\n",
    "            'question_type',\n",
    "            'multisession_index',\n",
    "            'session_index',\n",
    "            'message_index_within_session',\n",
    "            'message_index_across_sessions',\n",
    "            'session_date',\n",
    "            'message_role',\n",
    "            'num_previous_messages',\n",
    "            'message_has_answer',\n",
    "            'previous_message_1',\n",
    "            'previous_message_2',\n",
    "            'previous_message_3',\n",
    "            'previous_message_4',\n",
    "            'previous_message_5',\n",
    "            'message',\n",
    "        ]\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def insert_answer_columns(df, num_prompt_instructions):\n",
    "    for prompt_instruction_index in range(num_prompt_instructions, 0, -1):\n",
    "        for i in range(MAX_NUM_PREVIOUS_MESSAGES, 0, -1):\n",
    "            # Insert after each previous message\n",
    "            column_tag = f'({prompt_instruction_index}.{i})'\n",
    "            df.insert(\n",
    "                loc=df.columns.get_loc(f'previous_message_{i}') + 1,\n",
    "                column=f'Answer to Prompt Instruction {prompt_instruction_index} {column_tag}',\n",
    "                value='',\n",
    "            )\n",
    "            df.insert(\n",
    "                loc=df.columns.get_loc(f'previous_message_{i}') + 2,\n",
    "                column=f'Done?                {column_tag}',\n",
    "                value='',\n",
    "            )\n",
    "\n",
    "        column_tag = f'({prompt_instruction_index}.{MAX_NUM_PREVIOUS_MESSAGES + 1})'\n",
    "        # Insert after each previous message\n",
    "        df.insert(\n",
    "            loc=df.columns.get_loc(f'message') + 1,\n",
    "            column=f'Answer to Prompt Instruction {prompt_instruction_index} {column_tag}',\n",
    "            value='',\n",
    "        )\n",
    "        df.insert(\n",
    "            loc=df.columns.get_loc(f'message') + 2,\n",
    "            column=f'Done?                {column_tag}',\n",
    "            value='',\n",
    "        )\n",
    "\n",
    "\n",
    "def insert_default_answers_round1(df):\n",
    "    \"\"\"\n",
    "    Inserts default answers for the first round of prompt instructions.\n",
    "    \"\"\"\n",
    "    for i in range(MAX_NUM_PREVIOUS_MESSAGES, 0, -1):\n",
    "        column_tag = f'(1.{i})'\n",
    "        answer_col = f'Answer to Prompt Instruction 1 {column_tag}'\n",
    "        msg_col = f'previous_message_{i}'\n",
    "\n",
    "        # Set default value based on role from previous message\n",
    "        df[answer_col] = df[msg_col].apply(lambda x: f\"[${x['role']}$, ]\" if x is not None else '')\n",
    "\n",
    "    # Handle the final message\n",
    "    column_tag = f'(1.{MAX_NUM_PREVIOUS_MESSAGES + 1})'\n",
    "    answer_col = f'Answer to Prompt Instruction 1 {column_tag}'\n",
    "\n",
    "    # Set default value based on role from current message\n",
    "    df[answer_col] = df.apply(lambda row: f\"[${row['message_role']}$, ]\", axis=1)\n",
    "\n",
    "\n",
    "def insert_example_row(df, num_prompt_instructions):\n",
    "    \"\"\"\n",
    "    Inserts an example row at the top of the dataframe with 'EXAMPLE' as values.\n",
    "    \"\"\"\n",
    "    example_row = {col: 'EXAMPLE' for col in df.columns}\n",
    "    # for i in range(2):\n",
    "    #     for j in range(num_prompt_instructions):\n",
    "    #         example_row[f\"Done? ({j+1}.{i+1})\"] = \"x\"\n",
    "    df.loc[-1] = example_row\n",
    "    df.index = df.index + 1\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_eval_minidataset(df):\n",
    "    \"\"\"\n",
    "    Transforms the eval mini dataset so that there is a row for every message in previous messages.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.rename(columns={col: f'snippet_{col}' for col in df.columns})\n",
    "\n",
    "    ### Add new columns\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['snippet_index'] = df.index\n",
    "\n",
    "    transformed_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        previous_messages = json.loads(row['snippet_previous_messages'])\n",
    "\n",
    "        for i, message in enumerate(previous_messages):\n",
    "            new_row = row.copy()\n",
    "            new_row['message_index_within_snippet'] = i\n",
    "            new_row['input_message'] = json.dumps(message)\n",
    "            new_row['input_previous_messages'] = json.dumps(previous_messages[:i])\n",
    "            transformed_rows.append(new_row)\n",
    "\n",
    "        new_row = row.copy()\n",
    "        new_row['message_index_within_snippet'] = len(previous_messages)\n",
    "        new_row['input_message'] = row['snippet_message']\n",
    "        new_row['input_previous_messages'] = row['snippet_previous_messages']\n",
    "        transformed_rows.append(new_row)\n",
    "\n",
    "    transformed_df = pd.DataFrame(transformed_rows)\n",
    "\n",
    "    transformed_rows = []\n",
    "    task_names = [\n",
    "        'extract_nodes',\n",
    "        'dedupe_nodes',\n",
    "        'extract_edges',\n",
    "        'dedupe_edges',\n",
    "        'extract_edge_dates',\n",
    "        'edge_invalidation',\n",
    "    ]\n",
    "    for _, row in transformed_df.iterrows():\n",
    "        for task_index, task_name in enumerate(task_names):\n",
    "            new_row = row.copy()\n",
    "            new_row['task_name'] = task_name\n",
    "            new_row['task_index'] = task_index\n",
    "            transformed_rows.append(new_row)\n",
    "\n",
    "    transformed_df = pd.DataFrame(transformed_rows)\n",
    "\n",
    "    # Reorder columns\n",
    "    transformed_df = transformed_df[\n",
    "        [\n",
    "            'snippet_index',\n",
    "            'message_index_within_snippet',\n",
    "            'task_index',\n",
    "            'task_name',\n",
    "            'snippet_message',\n",
    "            'snippet_previous_messages',\n",
    "            'input_message',\n",
    "            'input_previous_messages',\n",
    "        ]\n",
    "    ]  # , 'input_extracted_nodes', 'input_existing_relevant_nodes', 'input_extracted_edges', 'input_existing_relevant_edges', 'output_zep', 'output_gpt4o', 'output_human']]\n",
    "\n",
    "    # Ensure to reset the indices to be sequential\n",
    "    transformed_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas settings to display all columns and have max width of columns\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Filtering to only snippets/rows we want\n",
    "lme_dataset_df_filtered = filter_for_zep_labelling(lme_dataset_df)\n",
    "lme_dataset_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Create the eval CSV\n",
    "eval_minidataset = lme_dataset_df_filtered.copy()\n",
    "eval_minidataset = transform_eval_minidataset(eval_minidataset)\n",
    "# Print the number of rows and columns\n",
    "print(f'Number of rows: {len(eval_minidataset)}')\n",
    "print(f'Number of columns: {len(eval_minidataset.columns)}')\n",
    "eval_minidataset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert gpt4o answers by doing ingestion in the right order and filling extra input columns as needed\n",
    "model_name = 'gpt-4o-mini'\n",
    "llm_config = LLMConfig(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    model=model_name,\n",
    ")\n",
    "llm_client = OpenAIClient(config=llm_config)\n",
    "output_column_name = 'output_gpt4o_mini'\n",
    "eval_minidataset_labelled = await ingest_and_label_minidataset(\n",
    "    llm_client, eval_minidataset, output_column_name\n",
    ")\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f'Number of rows: {len(eval_minidataset_labelled)}')\n",
    "print(f'Number of columns: {len(eval_minidataset_labelled.columns)}')\n",
    "eval_minidataset_labelled.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the input message column for the first row\n",
    "index = 2\n",
    "print('Input Message:')\n",
    "print(eval_minidataset_labelled.iloc[index]['input_message'])\n",
    "print('-' * 100)\n",
    "cell_value = eval_minidataset_labelled.iloc[index][output_column_name]\n",
    "cell_value_dicts = json.loads(cell_value)\n",
    "for dict in cell_value_dicts:\n",
    "    # Print only the 'fact' and 'name' values\n",
    "    print(f\"Fact: {dict.get('fact', 'N/A')}, Name: {dict.get('name', 'N/A')}\")\n",
    "    print('-' * 100)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Create the human labelling CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Create the human labelling CSV (old)\n",
    "\n",
    "\n",
    "# ######## Expanding the previous_messages column\n",
    "# lme_dataset_df_filtered_human_labelling = expand_previous_messages(lme_dataset_df_filtered)\n",
    "# lme_dataset_df_filtered_human_labelling.head()\n",
    "\n",
    "# ######## Order the columns in the way we want them\n",
    "# lme_dataset_df_filtered_human_labelling = order_columns(lme_dataset_df_filtered_human_labelling)\n",
    "# lme_dataset_df_filtered_human_labelling.head()\n",
    "\n",
    "# ######## Insert empty answer columns\n",
    "# num_prompt_instructions = 1\n",
    "# insert_answer_columns(lme_dataset_df_filtered_human_labelling, num_prompt_instructions)\n",
    "# lme_dataset_df_filtered_human_labelling.head()\n",
    "\n",
    "# ######## Insert default values for the answers\n",
    "# insert_default_answers_round1(lme_dataset_df_filtered_human_labelling)\n",
    "# lme_dataset_df_filtered_human_labelling.head()\n",
    "\n",
    "# ######## Make the messages more readable\n",
    "# lme_dataset_df_filtered_human_labelling = make_messages_readable(lme_dataset_df_filtered_human_labelling)\n",
    "# lme_dataset_df_filtered_human_labelling.head(10)\n",
    "\n",
    "# ######## Add example row to the top\n",
    "# insert_example_row(lme_dataset_df_filtered_human_labelling, num_prompt_instructions)\n",
    "# lme_dataset_df_filtered_human_labelling.head(10)\n",
    "\n",
    "# ######## Save to csv\n",
    "# lme_dataset_df_filtered_human_labelling.to_csv(\"lme_dataset_df_filtered_human_labelling.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
