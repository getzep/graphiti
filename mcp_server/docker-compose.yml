services:
  # Optional: Ollama service for LLM operations
  # Uncomment and use this if you want to run Ollama in a container
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/v1/models"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

  neo4j:
    image: neo4j:5.26.0
    ports:
      - "7474:7474" # HTTP
      - "7687:7687" # Bolt
    environment:
      - NEO4J_AUTH=${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-demodemo}
      - NEO4J_server_memory_heap_initial__size=512m
      - NEO4J_server_memory_heap_max__size=1G
      - NEO4J_server_memory_pagecache_size=512m
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "http://localhost:7474"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  graphiti-mcp:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    develop:
      watch:
        - path: ./src
          action: sync
          target: /app/src
        - path: ./pyproject.toml
          action: sync
          target: /app/pyproject.toml
        - path: ./uv.lock
          action: sync
          target: /app/uv.lock
        - path: ./Dockerfile
          action: rebuild
    env_file:
      - path: .env
        required: true
    depends_on:
      neo4j:
        condition: service_healthy
    environment:
      - NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-demodemo}
      - USE_OLLAMA=true
      - OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
      - OLLAMA_LLM_MODEL=deepseek-r1:7b
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text
      - OLLAMA_EMBEDDING_DIM=768
      - OPENAI_API_KEY=abc
      - PATH=/root/.local/bin:${PATH}
      - SEMAPHORE_LIMIT=${SEMAPHORE_LIMIT:-10}
      - MCP_SERVER_PORT=${MCP_SERVER_PORT:-8020}
      - MCP_INTERNAL_PORT=${MCP_INTERNAL_PORT:-$((MCP_SERVER_PORT + 1))}
      - OAUTH_CLIENT_ID=${OAUTH_CLIENT_ID:-graphiti-mcp}
      - OAUTH_CLIENT_SECRET=${OAUTH_CLIENT_SECRET:-graphiti-secret-key-change-this-in-production}
      - OAUTH_ISSUER=${OAUTH_ISSUER:-http://localhost:8020}
      - OAUTH_AUDIENCE=${OAUTH_AUDIENCE:-graphiti-mcp}
    ports:
      - "${MCP_SERVER_PORT:-8020}:8020" # Expose the OAuth wrapper
    command: ["sh", "-c", "uv run python src/graphiti_mcp_server.py --transport sse --port $MCP_INTERNAL_PORT & uv run python src/oauth_wrapper.py"]

volumes:
  neo4j_data:
  neo4j_logs:
  # ollama_data:  # Uncomment if using Ollama service
